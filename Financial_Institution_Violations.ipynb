{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import requests\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "from pandas import json_normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "\n",
    "file_exists = os.path.exists(\"inst_full.csv\")\n",
    "\n",
    "print(file_exists)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "THE CELL BEOW CHECKS TO SEE IF YOU ALREADY HAVE THE DATA AND IF NOT SENDS GET REQUESTS TO THE API\n",
    "\n",
    "NOTE - THIS DOES NOT CHECK TO ENSURE THE FILE IS CURRENT OR FREE OF ERRORS.  IF YOU WOULD LIKE TO UPDATE YOUR FILE OR YOU ARE CONCERNED THAT YOUR FILE HAS ERRORS, SIMPLY DELETE YOUR FILE AND A NEW, UPDATED FILE WILL BE CREATED."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last 20 lines in the cell below (most of the code in the ELSE statement) cleans the data by checking for columns with mixed data types and converting the entire column to strings.  It converts the data type, but also displays an error to identify the affected columns.  This is a solution I found on stackoverflow that I wanted to implement in my code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas\n",
    "\n",
    "\n",
    "if file_exists == False:\n",
    "    print(\"FILE DOES NOT EXIST\")\n",
    "\n",
    "    response_API = requests.get('https://banks.data.fdic.gov/api/institutions?limit=10000&offset=0')\n",
    "    #print(response_API.status_code)\n",
    "\n",
    "    response_API2 = requests.get('https://banks.data.fdic.gov/api/institutions?limit=10000&offset=10000')\n",
    "    #print(response_API2.status_code)\n",
    "\n",
    "    response_API3 = requests.get('https://banks.data.fdic.gov/api/institutions?limit=10000&offset=20000')\n",
    "    #print(response_API3.status_code)\n",
    "\n",
    "    print(\"Get request sucessful from API\")\n",
    "\n",
    "\n",
    "    data = response_API.json()\n",
    "    #print(type(data))\n",
    "\n",
    "    data2 = response_API2.json()\n",
    "    #print(type(data2))\n",
    "\n",
    "    data3 = response_API3.json()\n",
    "    #print(type(data3))\n",
    "\n",
    "\n",
    "\n",
    "    institution_df = json_normalize(data['data'])\n",
    "    #print(institution_df.head())\n",
    "    #institution_df.to_csv('inst.csv')\n",
    "\n",
    "    institution_df2 = json_normalize(data2['data'])\n",
    "    #print(institution_df2.head())\n",
    "    #institution_df2.to_csv('inst2.csv')\n",
    "\n",
    "    institution_df3 = json_normalize(data3['data'])\n",
    "    #print(institution_df3.head())\n",
    "    #institution_df3.to_csv('inst3.csv')\n",
    "    \n",
    "    print(\"Data from API converted to json\")\n",
    "\n",
    "\n",
    "    institution_df = institution_df.reindex(sorted(institution_df.columns), axis=1)\n",
    "    institution_df2 = institution_df2.reindex(sorted(institution_df2.columns), axis=1)\n",
    "    institution_df3 = institution_df3.reindex(sorted(institution_df3.columns), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "    full_data = [institution_df, institution_df2, institution_df3]\n",
    "    df = pd.concat(full_data)\n",
    "    df.columns = df.columns.str.lstrip(\"data.\")\n",
    "    df.to_csv('inst_full.csv')\n",
    "    \n",
    "    print(\"New file created, shape should be (27816, 168)\")\n",
    "    print(df.shape)\n",
    "    \n",
    "\n",
    "else:\n",
    "    print(\"FILE ALREADY EXISTS\")\n",
    "\n",
    "    myfile = 'inst_full.csv'\n",
    "    target_type = str  # The desired output type\n",
    "\n",
    "    with warnings.catch_warnings(record=True) as ws:\n",
    "        warnings.simplefilter(\"always\")\n",
    "\n",
    "        mydata = pandas.read_csv(myfile, sep=\"|\", header=None)\n",
    "        print(\"Warnings raised:\", ws)\n",
    "        # We have an error on specific columns, try and load them as string\n",
    "        for w in ws:\n",
    "            s = str(w.message)\n",
    "            print(\"Warning message:\", s)\n",
    "            match = re.search(r\"Columns \\(([0-9,]+)\\) have mixed types\\.\", s)\n",
    "            if match:\n",
    "                columns = match.group(1).split(',') # Get columns as a list\n",
    "                columns = [int(c) for c in columns]\n",
    "                print(\"Applying %s dtype to columns:\" % target_type, columns)\n",
    "                mydata.iloc[:,columns] = mydata.iloc[:,columns].astype(target_type)\n",
    "\n",
    "    df = pd.read_csv('inst_full.csv')\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".append is being deprecated - which is why I chose to use .concat instead"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reformating the column names so they are all lower case and spaces are replaced with an underscore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df.columns)\n",
    "df.columns= df.columns.str.lower()\n",
    "df.columns = df.columns.str.replace(' ', '_')\n",
    "#print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bank_count = len(df)\n",
    "print(\"There are\", bank_count, \"institutuions with data on the FDIC's API.\\n\")\n",
    "\n",
    "active = df['active'].value_counts()[0]\n",
    "print(\"There are\", active, \"active institutions registered with the FDIC.\\n\")\n",
    "\n",
    "inactive = df['active'].value_counts()[1]\n",
    "print(\"There are\", inactive, \"inactive institutions registered with the FDIC.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "active_df = df[df.active == 0]\n",
    "print(len(active_df))\n",
    "#print(active_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inactive_df = df[df.active == 1]\n",
    "print(len(inactive_df))\n",
    "#print(inactive_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min = \"{:,.2f}\".format(active_df[\"asset\"].min())\n",
    "max = \"{:,.2f}\".format(active_df[\"asset\"].max())\n",
    "mean = \"{:,.2f}\".format(active_df[\"asset\"].mean())\n",
    "\n",
    "print(\"The minimum assets held by an active FDIC registered institution are ${} \\n\".format(min))\n",
    "print(\"The maximum assets held by an active FDIC registered institution are ${} \\n\".format(max))\n",
    "print(\"The average assets held by an active FDIC registered institution are ${} \\n\".format(mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min = \"{:,.2f}\".format(inactive_df[\"asset\"].min())\n",
    "max = \"{:,.2f}\".format(inactive_df[\"asset\"].max())\n",
    "mean = \"{:,.2f}\".format(inactive_df[\"asset\"].mean())\n",
    "\n",
    "print(\"The minimum assets held by an inactive FDIC registered institution are ${} \\n\".format(min))\n",
    "print(\"The maximum assets held by an inactive FDIC registered institution are ${} \\n\".format(max))\n",
    "print(\"The average assets held by an inactive FDIC registered institution are ${} \\n\".format(mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min = \"{:,.2f}\".format(df[\"asset\"].min())\n",
    "max = \"{:,.2f}\".format(df[\"asset\"].max())\n",
    "mean = \"{:,.2f}\".format(df[\"asset\"].mean().round(2))\n",
    "\n",
    "print(\"The minimum assets held by an FDIC registered institution are ${} \\n\".format(min))\n",
    "print(\"The maximum assets held by an FDIC registered institution are ${} \\n\".format(max))\n",
    "print(\"The average assets held by an FDIC registered institution are ${} \\n\".format(mean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "Standarad deviation for later analysis\n",
    "\n",
    "std = df[\"data.ASSET\"].std().round(2)\n",
    "top_std = (mean + std).round(2)\n",
    "btm_std = (mean - std).round(2)\n",
    "\n",
    "print(std)\n",
    "print(\"Most FDIC registed banks have between ${} and ${} \\n\".format(btm_std, top_std))\n",
    "\n",
    "\n",
    "# to format as currency\n",
    "#.style.format('${0:,.2f}')\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To add banks with FDIC violations\n",
    "\n",
    "absolute_path = os.path.abspath('')\n",
    "relative_path = \"ots-enforcement-order-listing.csv\"\n",
    "full_path = os.path.join(absolute_path, relative_path)\n",
    "enforcment_df = pd.read_csv(full_path)\n",
    "print(len(enforcment_df))\n",
    "#print(enforcment_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(enforcment_df.columns)\n",
    "enforcment_df.columns= enforcment_df.columns.str.lower()\n",
    "enforcment_df.columns = enforcment_df.columns.str.replace(' ', '_')\n",
    "#print(enforcment_df.columns)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next two cells remove improperly formatted docket numbers and then remove rows without docket numbers from the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "enforcment_df['docket_number'] = np.where(enforcment_df['docket_number'].str.startswith('H'), '', enforcment_df['docket_number'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new df so the old df is unaltered\n",
    "\n",
    "#sorts the new df by docket number\n",
    "enf_df1 = enforcment_df.sort_values('docket_number')\n",
    "\n",
    "#converts emtpy strings to NaN and then drops rows that do not have a Docket Number\n",
    "enf_df1['docket_number'].replace('', np.nan, inplace=True)\n",
    "enf_df1.dropna(subset=['docket_number'], inplace=True)\n",
    "\n",
    "#Gives the size and first columns of the new_enf_df\n",
    "print(len(enf_df1.sort_values('docket_number')))\n",
    "#print(enf_df1.head(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drops duplicates and adds 2 new colums to the new_enf_df\n",
    "\n",
    "short_enf_df = enf_df1.drop_duplicates(subset=['docket_number'], keep='last')\n",
    "#short_enf_df = short_enf_df.reindex(columns = new_enf_df.columns.tolist() + ['Number_of_Violations','Unique_Violations'])\n",
    "\n",
    "#Gives the size and first columns of the short_enf_df\n",
    "print(len(short_enf_df))\n",
    "#print(short_enf_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violation_count_df = enf_df1.pivot_table(index = ['docket_number'], aggfunc ='size')\n",
    "vc_df = violation_count_df.reset_index()\n",
    "vc_df.columns = ['docket_number', 'total_violations']\n",
    "#print(vc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_violation_count_df = enf_df1.pivot_table(index = ['docket_number', \"issue_date\"], aggfunc ='size')\n",
    "uvc_df = unique_violation_count_df.reset_index()\n",
    "uvc_df.columns = ['docket_number', 'issue_date', 'violations']\n",
    "#print(uvc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_uvc_df = uvc_df.pivot_table(index = ['docket_number'], aggfunc ='size')\n",
    "fvc_df = violation_count_df.reset_index()\n",
    "fvc_df.columns = ['docket_number', 'unique_violations']\n",
    "#print(fvc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "violation_stats_df = vc_df.merge(fvc_df)\n",
    "#print(violation_stats_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_enf_df = short_enf_df.merge(violation_stats_df)\n",
    "#print(final_enf_df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['docket'] = df['docket'].astype(str)\n",
    "final_df = df.merge(final_enf_df, how='left', left_on='docket', right_on='docket_number')\n",
    "small_final_df = final_df[['asset', 'chrtagnt', 'clcode', 'effdate', 'fdicregn', 'name', 'trust', 'total_violations', 'unique_violations']].copy()\n",
    "#print(small_final_df.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_final_df['total_violations'] = final_df['total_violations'].fillna(0)\n",
    "small_final_df['unique_violations'] = final_df['unique_violations'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_violation_df = small_final_df[small_final_df.total_violations == 0]\n",
    "violation_df = small_final_df[small_final_df.total_violations != 0]\n",
    "\n",
    "print(no_violation_df.shape[0])\n",
    "print(violation_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_final_df.to_json('final.json', orient='records', lines=True)\n",
    "no_violation_df.to_json('no_violation.json', orient='records', lines=True)\n",
    "violation_df.to_json('violation.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "\n",
    "# plot a Stacked Bar Chart using matplotlib\n",
    "small_final_df.plot(\n",
    "  x = 'fdicregn', \n",
    "  kind = 'bar', \n",
    "  stacked = True, \n",
    "  title = 'Percentage Stacked Bar Graph', \n",
    "  mark_right = True) \n",
    "\n",
    "\"\"\"\n",
    "df_total = df[\"Studied\"] + df[\"Slept\"] + df[\"Other\"]\n",
    "df_rel = df[df.columns[1:]].div(df_total, 0)*100\n",
    "  \n",
    "for n in df_rel:\n",
    "    for i, (cs, ab, pc) in enumerate(zip(df.iloc[:, 1:].cumsum(1)[n], \n",
    "                                         df[n], df_rel[n])):\n",
    "        plt.text(cs - ab / 2, i, str(np.round(pc, 1)) + '%', \n",
    "                 va = 'center', ha = 'center')\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c0589475581bafe2bbc330412b8cf2cd44fed02eacf7679fc9db622cc240f316"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
